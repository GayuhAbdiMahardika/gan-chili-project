{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8c968ef",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/GAN_Chili_Disease/blob/main/colab_setup.ipynb)\n",
    "\n",
    "# üéÜ GAN + GitHub = Perfect Workflow!\n",
    "\n",
    "**üöÄ New Feature**: Sekarang dengan **GitHub Integration**! \n",
    "Tidak perlu upload-download file lagi. Clone sekali, training langsung, hasil auto-sync ke GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb2dd6e",
   "metadata": {},
   "source": [
    "# üå∂Ô∏è GAN Data Augmentation untuk Klasifikasi Penyakit Tanaman Cabai\n",
    "\n",
    "Notebook ini untuk menjalankan training GAN di Google Colab dengan optimasi GPU.\n",
    "\n",
    "**Estimasi waktu training:**\n",
    "- Demo (1 kelas): 15-20 menit\n",
    "- Full training (5 kelas): 3-4 jam\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab Pro (untuk GPU T4 atau lebih baik)\n",
    "- Dataset penyakit tanaman cabai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786ba8cd",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371254aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU not available, will use CPU (slower training)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a0c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install Pillow matplotlib tqdm seaborn opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8501dc90",
   "metadata": {},
   "source": [
    "## üìÅ Setup Dataset & Code\n",
    "\n",
    "**Opsi 1: Clone dari GitHub (Recommended)**\n",
    "- Push proyek Anda ke GitHub repository\n",
    "- Clone langsung di Colab dengan satu perintah\n",
    "- Auto-sync dengan perubahan terbaru\n",
    "\n",
    "**Opsi 2: Upload ZIP file**\n",
    "- Compress folder dataset Anda menjadi ZIP\n",
    "- Upload dan extract di cell berikutnya\n",
    "\n",
    "**Opsi 3: Google Drive**\n",
    "- Upload dataset ke Google Drive\n",
    "- Mount drive dan copy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e65452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: Clone from GitHub (RECOMMENDED)\n",
    "# First, push your project to GitHub, then clone here\n",
    "\n",
    "# Clone your repository\n",
    "# !git clone https://github.com/YOUR_USERNAME/YOUR_REPO_NAME.git\n",
    "# %cd YOUR_REPO_NAME\n",
    "\n",
    "# For this specific project, uncomment and modify:\n",
    "!git clone https://github.com/YOUR_USERNAME/GAN_Chili_Disease.git\n",
    "%cd GAN_Chili_Disease\n",
    "\n",
    "print(\"‚úÖ Repository cloned successfully!\")\n",
    "print(\"üìÅ Current directory:\")\n",
    "!pwd\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a218438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy dataset from Drive (adjust path as needed)\n",
    "# !cp -r \"/content/drive/MyDrive/Dataset Original\" ./\n",
    "print(\"Dataset copied from Google Drive\")\n",
    "\n",
    "# OPTION 2: Upload ZIP file (Alternative)\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "print(\"Upload your dataset ZIP file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Extract ZIP\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.zip'):\n",
    "        print(f\"Extracting {filename}...\")\n",
    "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall('.')\n",
    "        break\n",
    "\n",
    "# List extracted contents\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c02960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 3: Mount Google Drive (Alternative)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy dataset from Drive (adjust path as needed)\n",
    "# !cp -r \"/content/drive/MyDrive/Dataset Original\" ./\n",
    "print(\"Dataset copied from Google Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072d57e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure\n",
    "import os\n",
    "\n",
    "def check_dataset_structure():\n",
    "    \"\"\"Check dataset structure\"\"\"\n",
    "    dataset_paths = [\n",
    "        \"Dataset Original/train\",  # GitHub clone structure\n",
    "        \"data/train\",              # Alternative structure\n",
    "        \"train\",                   # Direct upload\n",
    "        \"/content/Dataset Original/train\"  # Full path\n",
    "    ]\n",
    "    \n",
    "    dataset_path = None\n",
    "    for path in dataset_paths:\n",
    "        if os.path.exists(path):\n",
    "            dataset_path = path\n",
    "            break\n",
    "    \n",
    "    if not dataset_path:\n",
    "        print(\"‚ùå Dataset not found! Please:\")\n",
    "        print(\"   1. Clone from GitHub with dataset, OR\")\n",
    "        print(\"   2. Upload dataset ZIP file\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"‚úÖ Dataset found at: {dataset_path}\")\n",
    "    \n",
    "    classes = ['healthy', 'leaf curl', 'leaf spot', 'whitefly', 'yellowish']\n",
    "    total_images = 0\n",
    "    \n",
    "    print(\"\\nDataset summary:\")\n",
    "    for class_name in classes:\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        if os.path.exists(class_path):\n",
    "            count = len([f for f in os.listdir(class_path) \n",
    "                        if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "            total_images += count\n",
    "            print(f\"  {class_name:12}: {count:3d} images\")\n",
    "        else:\n",
    "            print(f\"  {class_name:12}: Not found\")\n",
    "    \n",
    "    print(f\"\\nTotal images: {total_images}\")\n",
    "    return dataset_path\n",
    "\n",
    "# Check if we're in a git repository\n",
    "if os.path.exists('.git'):\n",
    "    print(\"üîó Git repository detected!\")\n",
    "    !git status\n",
    "else:\n",
    "    print(\"üìÅ Not a git repository\")\n",
    "\n",
    "DATASET_PATH = check_dataset_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fefa3a9",
   "metadata": {},
   "source": [
    "## üîÑ Git Operations & Sync\n",
    "\n",
    "Jika menggunakan GitHub, Anda bisa sync perubahan dan push hasil training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282a2eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Git operations untuk sync dengan GitHub\n",
    "def setup_git_config():\n",
    "    \"\"\"Setup Git configuration\"\"\"\n",
    "    # Configure git (replace with your info)\n",
    "    !git config --global user.name \"Your Name\"\n",
    "    !git config --global user.email \"your.email@example.com\"\n",
    "    print(\"‚úÖ Git configured successfully!\")\n",
    "\n",
    "def pull_latest_changes():\n",
    "    \"\"\"Pull latest changes from GitHub\"\"\"\n",
    "    try:\n",
    "        !git pull origin main\n",
    "        print(\"‚úÖ Latest changes pulled successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Pull failed: {e}\")\n",
    "\n",
    "def push_results():\n",
    "    \"\"\"Push training results back to GitHub\"\"\"\n",
    "    try:\n",
    "        # Add results to git\n",
    "        !git add colab_models/ colab_samples/ colab_augmented/\n",
    "        !git commit -m \"Add Colab training results - $(date)\"\n",
    "        !git push origin main\n",
    "        print(\"‚úÖ Results pushed to GitHub successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Push failed: {e}\")\n",
    "        print(\"You may need to setup GitHub authentication\")\n",
    "\n",
    "def check_git_status():\n",
    "    \"\"\"Check current git status\"\"\"\n",
    "    if os.path.exists('.git'):\n",
    "        print(\"üîó Git Repository Status:\")\n",
    "        !git status\n",
    "        print(\"\\nüåø Recent commits:\")\n",
    "        !git log --oneline -5\n",
    "    else:\n",
    "        print(\"‚ùå Not a git repository\")\n",
    "\n",
    "# Check current status\n",
    "check_git_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c33567c",
   "metadata": {},
   "source": [
    "## ü§ñ GAN Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c53b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters for Colab\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 64 if torch.cuda.is_available() else 32  # Larger batch for GPU\n",
    "NUM_EPOCHS = 200  # Reduced for Colab time limits\n",
    "LEARNING_RATE = 0.0002\n",
    "BETA1 = 0.5\n",
    "NZ = 100  # Size of latent vector\n",
    "NGF = 64  # Generator feature map size\n",
    "NDF = 64  # Discriminator feature map size\n",
    "NC = 3    # Number of channels\n",
    "\n",
    "class ChiliDataset(Dataset):\n",
    "    \"\"\"Custom dataset untuk memuat gambar cabai\"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        \n",
    "        # Load semua gambar dari folder\n",
    "        for file in os.listdir(root_dir):\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                self.images.append(os.path.join(root_dir, file))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image\n",
    "\n",
    "def weights_init(m):\n",
    "    \"\"\"Initialize weights for neural networks\"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, ngf, nc):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nc, ndf):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "print(\"‚úÖ GAN models defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5091b3f4",
   "metadata": {},
   "source": [
    "## üöÄ Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a3567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_colab(data_dir, class_name, target_images=200, epochs=200):\n",
    "    \"\"\"\n",
    "    Train GAN optimized for Google Colab\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Training GAN untuk kelas: {class_name} ===\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Epochs: {epochs}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    \n",
    "    # Data transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = ChiliDataset(data_dir, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    \n",
    "    print(f\"Jumlah gambar asli: {len(dataset)}\")\n",
    "    print(f\"Target gambar: {target_images}\")\n",
    "    print(f\"Perlu generate: {target_images - len(dataset)} gambar\")\n",
    "    \n",
    "    # Initialize networks\n",
    "    netG = Generator(NZ, NGF, NC).to(device)\n",
    "    netD = Discriminator(NC, NDF).to(device)\n",
    "    \n",
    "    # Apply weight initialization\n",
    "    netG.apply(weights_init)\n",
    "    netD.apply(weights_init)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # Optimizers\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=LEARNING_RATE, betas=(BETA1, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=LEARNING_RATE, betas=(BETA1, 0.999))\n",
    "    \n",
    "    # Fixed noise for monitoring progress\n",
    "    fixed_noise = torch.randn(16, NZ, 1, 1, device=device)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"Starting Training...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "    \n",
    "    # Progress bar for epochs\n",
    "    epoch_pbar = tqdm(range(epochs), desc=f\"Training {class_name}\")\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            ############################\n",
    "            # (1) Update D network\n",
    "            ############################\n",
    "            netD.zero_grad()\n",
    "            \n",
    "            # Train with real batch\n",
    "            real_batch = data.to(device)\n",
    "            b_size = real_batch.size(0)\n",
    "            label = torch.full((b_size,), 1., dtype=torch.float, device=device)\n",
    "            \n",
    "            output = netD(real_batch).view(-1)\n",
    "            errD_real = criterion(output, label)\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "            \n",
    "            # Train with fake batch\n",
    "            noise = torch.randn(b_size, NZ, 1, 1, device=device)\n",
    "            fake = netG(noise)\n",
    "            label.fill_(0.)\n",
    "            \n",
    "            output = netD(fake.detach()).view(-1)\n",
    "            errD_fake = criterion(output, label)\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item()\n",
    "            \n",
    "            errD = errD_real + errD_fake\n",
    "            optimizerD.step()\n",
    "            \n",
    "            ############################\n",
    "            # (2) Update G network\n",
    "            ###########################\n",
    "            netG.zero_grad()\n",
    "            label.fill_(1.)\n",
    "            \n",
    "            output = netD(fake).view(-1)\n",
    "            errG = criterion(output, label)\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            optimizerG.step()\n",
    "        \n",
    "        # Save losses\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        \n",
    "        # Update progress bar\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        epoch_pbar.set_postfix({\n",
    "            'D_loss': f\"{errD.item():.3f}\",\n",
    "            'G_loss': f\"{errG.item():.3f}\",\n",
    "            'Time': f\"{epoch_time:.1f}s\"\n",
    "        })\n",
    "        \n",
    "        # Save sample images periodically\n",
    "        if epoch % 25 == 0 or epoch == epochs - 1:\n",
    "            with torch.no_grad():\n",
    "                fake_sample = netG(fixed_noise).detach().cpu()\n",
    "                sample_dir = f\"colab_samples/{class_name}\"\n",
    "                os.makedirs(sample_dir, exist_ok=True)\n",
    "                vutils.save_image(fake_sample, f\"{sample_dir}/epoch_{epoch}.png\", \n",
    "                                normalize=True, nrow=4)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ Training completed in {total_time/60:.1f} minutes\")\n",
    "    \n",
    "    # Save model\n",
    "    model_dir = f\"colab_models/{class_name}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    torch.save(netG.state_dict(), f\"{model_dir}/generator.pth\")\n",
    "    torch.save(netD.state_dict(), f\"{model_dir}/discriminator.pth\")\n",
    "    \n",
    "    print(f\"Model saved to {model_dir}\")\n",
    "    \n",
    "    return netG, netD, G_losses, D_losses\n",
    "\n",
    "def generate_images_colab(generator, class_name, num_images, output_dir=\"colab_augmented\"):\n",
    "    \"\"\"Generate images using trained generator\"\"\"\n",
    "    print(f\"\\nGenerating {num_images} images for {class_name}...\")\n",
    "    \n",
    "    generator.eval()\n",
    "    os.makedirs(f\"{output_dir}/{class_name}\", exist_ok=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(num_images), desc=\"Generating\"):\n",
    "            noise = torch.randn(1, NZ, 1, 1, device=device)\n",
    "            fake_image = generator(noise)\n",
    "            \n",
    "            # Convert to PIL and save\n",
    "            img = fake_image[0].cpu()\n",
    "            img = (img + 1) / 2.0  # Denormalize\n",
    "            img = transforms.ToPILImage()(img)\n",
    "            \n",
    "            img_name = f\"generated_{class_name.replace(' ', '_')}_{i:03d}.jpg\"\n",
    "            img_path = os.path.join(f\"{output_dir}/{class_name}\", img_name)\n",
    "            img.save(img_path)\n",
    "    \n",
    "    print(f\"‚úÖ Generated {num_images} images saved to {output_dir}/{class_name}\")\n",
    "\n",
    "def plot_training_losses(G_losses, D_losses, class_name):\n",
    "    \"\"\"Plot training losses\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(G_losses, label='Generator', color='blue')\n",
    "    plt.plot(D_losses, label='Discriminator', color='red')\n",
    "    plt.title(f'Training Losses - {class_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Show latest sample\n",
    "    sample_path = f\"colab_samples/{class_name}\"\n",
    "    if os.path.exists(sample_path):\n",
    "        sample_files = [f for f in os.listdir(sample_path) if f.endswith('.png')]\n",
    "        if sample_files:\n",
    "            latest_sample = max(sample_files, key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "            img = plt.imread(os.path.join(sample_path, latest_sample))\n",
    "            plt.imshow(img)\n",
    "            plt.title(f'Latest Generated Samples - {class_name}')\n",
    "            plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Training functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88558727",
   "metadata": {},
   "source": [
    "## üéØ Demo Training (Single Class)\n",
    "\n",
    "Mulai dengan demo training untuk satu kelas terlebih dahulu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6c4939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo training untuk satu kelas\n",
    "if DATASET_PATH:\n",
    "    # Pilih kelas untuk demo\n",
    "    demo_class = \"healthy\"  # Ganti dengan kelas yang tersedia\n",
    "    demo_epochs = 50        # Epoch minimal untuk demo\n",
    "    \n",
    "    demo_data_dir = os.path.join(DATASET_PATH, demo_class)\n",
    "    \n",
    "    if os.path.exists(demo_data_dir):\n",
    "        print(f\"üöÄ Starting demo training for class: {demo_class}\")\n",
    "        print(f\"üìÅ Data directory: {demo_data_dir}\")\n",
    "        print(f\"üîÑ Epochs: {demo_epochs}\")\n",
    "        \n",
    "        # Start training\n",
    "        generator, discriminator, g_losses, d_losses = train_gan_colab(\n",
    "            demo_data_dir, demo_class, target_images=120, epochs=demo_epochs\n",
    "        )\n",
    "        \n",
    "        # Plot losses\n",
    "        plot_training_losses(g_losses, d_losses, demo_class)\n",
    "        \n",
    "        # Generate sample images\n",
    "        generate_images_colab(generator, demo_class, 20)\n",
    "        \n",
    "        print(f\"\\nüéâ Demo completed for {demo_class}!\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Class directory not found: {demo_data_dir}\")\n",
    "        print(\"Available classes:\")\n",
    "        for item in os.listdir(DATASET_PATH):\n",
    "            if os.path.isdir(os.path.join(DATASET_PATH, item)):\n",
    "                print(f\"  - {item}\")\n",
    "else:\n",
    "    print(\"‚ùå Please upload dataset first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab64af4d",
   "metadata": {},
   "source": [
    "## üè≠ Full Training (All Classes)\n",
    "\n",
    "Setelah demo berhasil, jalankan training untuk semua kelas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a2ffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training untuk semua kelas\n",
    "def train_all_classes_colab(dataset_path, epochs=150, target_images=200):\n",
    "    \"\"\"Train GAN for all classes\"\"\"\n",
    "    classes = ['healthy', 'leaf curl', 'leaf spot', 'whitefly', 'yellowish']\n",
    "    \n",
    "    print(\"üè≠ FULL TRAINING - ALL CLASSES\")\n",
    "    print(f\"üìä Target per class: {target_images} images\")\n",
    "    print(f\"üîÑ Training epochs: {epochs}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    results = {}\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for i, class_name in enumerate(classes, 1):\n",
    "        print(f\"\\n[{i}/{len(classes)}] Training class: {class_name}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        class_dir = os.path.join(dataset_path, class_name)\n",
    "        \n",
    "        if not os.path.exists(class_dir):\n",
    "            print(f\"‚ö†Ô∏è  Directory not found: {class_dir}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Train GAN for this class\n",
    "            generator, discriminator, g_losses, d_losses = train_gan_colab(\n",
    "                class_dir, class_name, target_images, epochs\n",
    "            )\n",
    "            \n",
    "            # Generate images\n",
    "            needed_images = target_images - len([f for f in os.listdir(class_dir) \n",
    "                                               if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "            \n",
    "            if needed_images > 0:\n",
    "                generate_images_colab(generator, class_name, needed_images)\n",
    "            \n",
    "            # Plot losses\n",
    "            plot_training_losses(g_losses, d_losses, class_name)\n",
    "            \n",
    "            results[class_name] = {\n",
    "                'generator': generator,\n",
    "                'discriminator': discriminator,\n",
    "                'g_losses': g_losses,\n",
    "                'd_losses': d_losses\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ Completed class {class_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error training {class_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    total_time = time.time() - total_start_time\n",
    "    print(f\"\\nüéä ALL TRAINING COMPLETED!\")\n",
    "    print(f\"‚è∞ Total time: {total_time/3600:.1f} hours\")\n",
    "    print(f\"üìÅ Results saved in:\")\n",
    "    print(f\"   - colab_models/     : Trained models\")\n",
    "    print(f\"   - colab_augmented/  : Generated images\")\n",
    "    print(f\"   - colab_samples/    : Training samples\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run full training (uncomment to start)\n",
    "# WARNING: This will take 3-4 hours\n",
    "# results = train_all_classes_colab(DATASET_PATH, epochs=150, target_images=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf86f9",
   "metadata": {},
   "source": [
    "## üìä Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f86db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_grid(original_dir, generated_dir, class_name, num_samples=8):\n",
    "    \"\"\"Create comparison grid of original vs generated images\"\"\"\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(20, 6))\n",
    "    \n",
    "    # Original images\n",
    "    orig_files = [f for f in os.listdir(original_dir) \n",
    "                  if f.lower().endswith(('.png', '.jpg', '.jpeg'))][:num_samples]\n",
    "    \n",
    "    for i, filename in enumerate(orig_files[:num_samples]):\n",
    "        img_path = os.path.join(original_dir, filename)\n",
    "        img = plt.imread(img_path)\n",
    "        axes[0, i].imshow(img)\n",
    "        axes[0, i].set_title('Original', fontsize=10)\n",
    "        axes[0, i].axis('off')\n",
    "    \n",
    "    # Generated images\n",
    "    gen_files = [f for f in os.listdir(generated_dir) \n",
    "                 if f.lower().endswith(('.png', '.jpg', '.jpeg'))][:num_samples]\n",
    "    \n",
    "    for i, filename in enumerate(gen_files[:num_samples]):\n",
    "        img_path = os.path.join(generated_dir, filename)\n",
    "        img = plt.imread(img_path)\n",
    "        axes[1, i].imshow(img)\n",
    "        axes[1, i].set_title('Generated', fontsize=10)\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Original vs Generated - {class_name}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_training_progress():\n",
    "    \"\"\"Show training progress for all classes\"\"\"\n",
    "    classes = ['healthy', 'leaf curl', 'leaf spot', 'whitefly', 'yellowish']\n",
    "    \n",
    "    print(\"üìà TRAINING PROGRESS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for class_name in classes:\n",
    "        model_dir = f\"colab_models/{class_name}\"\n",
    "        sample_dir = f\"colab_samples/{class_name}\"\n",
    "        gen_dir = f\"colab_augmented/{class_name}\"\n",
    "        \n",
    "        status = \"‚ùå Not trained\"\n",
    "        if os.path.exists(model_dir) and os.path.exists(f\"{model_dir}/generator.pth\"):\n",
    "            status = \"‚úÖ Completed\"\n",
    "            \n",
    "            # Count generated images\n",
    "            if os.path.exists(gen_dir):\n",
    "                gen_count = len([f for f in os.listdir(gen_dir) \n",
    "                               if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "                status += f\" ({gen_count} images)\"\n",
    "        \n",
    "        print(f\"  {class_name:15}: {status}\")\n",
    "    \n",
    "    print(\"\\nüìÅ Output directories:\")\n",
    "    print(f\"   - colab_models/     : Trained GAN models\")\n",
    "    print(f\"   - colab_augmented/  : Generated images for augmentation\")\n",
    "    print(f\"   - colab_samples/    : Training progress samples\")\n",
    "\n",
    "# Show current progress\n",
    "show_training_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612bcdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison grids for trained classes\n",
    "def show_all_comparisons():\n",
    "    \"\"\"Show comparison grids for all trained classes\"\"\"\n",
    "    classes = ['healthy', 'leaf curl', 'leaf spot', 'whitefly', 'yellowish']\n",
    "    \n",
    "    for class_name in classes:\n",
    "        original_dir = os.path.join(DATASET_PATH, class_name) if DATASET_PATH else None\n",
    "        generated_dir = f\"colab_augmented/{class_name}\"\n",
    "        \n",
    "        if (original_dir and os.path.exists(original_dir) and \n",
    "            os.path.exists(generated_dir)):\n",
    "            \n",
    "            print(f\"\\nüìä Comparison for {class_name}:\")\n",
    "            create_comparison_grid(original_dir, generated_dir, class_name)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Cannot create comparison for {class_name} - missing data\")\n",
    "\n",
    "# Show comparisons (uncomment after training)\n",
    "# show_all_comparisons()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e2ecff",
   "metadata": {},
   "source": [
    "## üíæ Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433b7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ZIP file for download\n",
    "import zipfile\n",
    "from google.colab import files\n",
    "\n",
    "def create_results_zip():\n",
    "    \"\"\"Create ZIP file with all results\"\"\"\n",
    "    zip_filename = \"gan_results.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Add generated images\n",
    "        if os.path.exists(\"colab_augmented\"):\n",
    "            for root, dirs, files in os.walk(\"colab_augmented\"):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    zipf.write(file_path, os.path.relpath(file_path))\n",
    "        \n",
    "        # Add trained models\n",
    "        if os.path.exists(\"colab_models\"):\n",
    "            for root, dirs, files in os.walk(\"colab_models\"):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    zipf.write(file_path, os.path.relpath(file_path))\n",
    "        \n",
    "        # Add sample images\n",
    "        if os.path.exists(\"colab_samples\"):\n",
    "            for root, dirs, files in os.walk(\"colab_samples\"):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    zipf.write(file_path, os.path.relpath(file_path))\n",
    "    \n",
    "    print(f\"‚úÖ Results packed into {zip_filename}\")\n",
    "    return zip_filename\n",
    "\n",
    "def download_results():\n",
    "    \"\"\"Download all results\"\"\"\n",
    "    try:\n",
    "        zip_file = create_results_zip()\n",
    "        files.download(zip_file)\n",
    "        print(\"üì• Download started!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Download error: {str(e)}\")\n",
    "\n",
    "# Download results (uncomment after training)\n",
    "# download_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffc9740",
   "metadata": {},
   "source": [
    "## üìù Summary & GitHub Workflow\n",
    "\n",
    "### Setup Workflow dengan GitHub:\n",
    "\n",
    "#### 1. **Persiapan Awal** (Sekali saja):\n",
    "```bash\n",
    "# Di komputer lokal:\n",
    "git init\n",
    "git add .\n",
    "git commit -m \"Initial commit\"\n",
    "git branch -M main\n",
    "git remote add origin https://github.com/USERNAME/REPO_NAME.git\n",
    "git push -u origin main\n",
    "```\n",
    "\n",
    "#### 2. **Di Google Colab** (Setiap session):\n",
    "- Clone repository: `!git clone https://github.com/USERNAME/REPO_NAME.git`\n",
    "- Jalankan training\n",
    "- Push hasil: `!git add . && git commit -m \"Training results\" && git push`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "5. **Use .gitignore** untuk file besar (models opsional)4. **Create branches** untuk eksperimen berbeda3. **Use meaningful commit messages** untuk tracking2. **Commit frequently** untuk backup progress1. **Setup GitHub** dengan SSH keys untuk push yang mudah### Tips untuk workflow optimal:- `.git/`: Version control history- `colab_samples/`: Sample progress training- `colab_augmented/`: Gambar hasil augmentasi  - `colab_models/`: Model GAN terlatih (generator & discriminator)### File yang dihasilkan:4. **Gunakan** dataset augmented untuk training model klasifikasi3. **Gabungkan** dengan dataset asli untuk membuat dataset augmented2. **Download local** jika perlu: `git pull` di komputer1. **Push hasil** ke GitHub menggunakan cell Git operations### Setelah training selesai:- ‚úÖ **Reproducible**: Environment dan code terdokumentasi- ‚úÖ **Collaboration**: Tim bisa akses yang sama- ‚úÖ **Backup otomatis**: Hasil training tersimpan aman- ‚úÖ **Version control**: Track semua perubahan code- ‚úÖ **No upload berulang**: Clone sekali, otomatis latest#### 3. **Keuntungan GitHub Integration**:\n",
    "## üéÜ Complete GitHub Workflow Summary\n",
    "\n",
    "### üöÄ **Quick Start with GitHub**:\n",
    "\n",
    "```python\n",
    "# 1. Clone your repository\n",
    "!git clone https://github.com/USERNAME/REPO_NAME.git\n",
    "%cd REPO_NAME\n",
    "\n",
    "# 2. Run training\n",
    "# ... (training cells) ...\n",
    "\n",
    "# 3. Backup results\n",
    "manual_backup_now()\n",
    "\n",
    "# 4. Optional: Auto-backup during long training\n",
    "auto_backup_training(30)  # Every 30 minutes\n",
    "```\n",
    "\n",
    "### üåø **Benefits of GitHub Integration**:\n",
    "\n",
    "| Feature | Without GitHub | With GitHub |\n",
    "|---------|----------------|-------------|\n",
    "| **File Access** | Upload setiap kali | Clone sekali aja |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Sekarang Anda tidak perlu upload-download file lagi! Semua tersync otomatis dengan GitHub. Perfect untuk thesis work! üìöüöÄ\n",
    "### üéâ **Result**: - **Document** semua hyperparameters di README\n",
    "- Keep **large files** di Git LFS atau external storage- Use **branches** untuk eksperimen berbeda- Setup **auto-backup** untuk training lama- Use **meaningful commit messages** untuk track progress### ‚ú® **Pro Tips**:```‚îî‚îÄ‚îÄ .gitignore          # Git ignore rules‚îú‚îÄ‚îÄ colab_augmented/    # Generated images (auto-generated)‚îú‚îÄ‚îÄ colab_samples/      # Training samples (auto-generated)‚îú‚îÄ‚îÄ colab_models/       # Trained models (auto-generated)‚îú‚îÄ‚îÄ Dataset Original/   # Sample data‚îú‚îÄ‚îÄ requirements_colab.txt‚îú‚îÄ‚îÄ COLAB_GUIDE.md      # Documentation‚îú‚îÄ‚îÄ colab_setup.ipynb   # Main Colab notebook‚îú‚îÄ‚îÄ src/                 # Source codeGAN_Chili_Disease/```\n",
    "\n",
    "### üìù **Repository Structure**:   - Download results dari GitHub ke local   - Monitor progress via GitHub commits\n",
    "\n",
    "   - Setup auto-backup setiap 30 menit3. **Long Training** (3-4 jam):   - Push final results   - Auto-backup progress   - Run training   - Pull latest changes\n",
    "   - Clone repo di Colab baru2. **Daily Usage** (Setiap training):   - Setup authentication   - Push local project ke GitHub   - Buat GitHub repo1. **Setup** (Pertama kali):\n",
    "### üîÑ **Typical Workflow**:| **Version Control** | Manual backup | Otomatis track changes |\n",
    "| **Collaboration** | Share files manual | Real-time collaboration |\n",
    "| **Backup** | Download manual | Auto-push ke cloud |\n",
    "| **Reproducibility** | Setup manual | Environment terdokumentasi |\n",
    "| **History** | Tidak ada | Full change history |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569a1e04",
   "metadata": {},
   "source": [
    "## üöÄ Setup GitHub Repository\n",
    "\n",
    "### Langkah-langkah Setup GitHub (First Time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0886e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ CREATE GITHUB REPOSITORY\n",
    "# Run this in your LOCAL computer first, then use in Colab\n",
    "\n",
    "def show_github_setup_guide():\n",
    "    \"\"\"Show step-by-step GitHub setup guide\"\"\"\n",
    "    \n",
    "    print(\"üöÄ GITHUB SETUP GUIDE\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"\\n1Ô∏è‚É£ CREATE GITHUB REPOSITORY:\")\n",
    "    print(\"   - Go to github.com\")\n",
    "    print(\"   - Click 'New Repository'\")\n",
    "    print(\"   - Name: 'GAN_Chili_Disease' (or any name)\")\n",
    "    print(\"   - Make it PUBLIC or PRIVATE\")\n",
    "    print(\"   - Don't initialize with README (we have files already)\")\n",
    "    \n",
    "    print(\"\\n2Ô∏è‚É£ LOCAL SETUP (Run in your computer):\")\n",
    "    print(\"   cd 'c:\\\\Riset Infromatika\\\\Python V3\\\\GAN_Project'\")\n",
    "    print(\"   git init\")\n",
    "    print(\"   git add .\")\n",
    "    print(\"   git commit -m 'Initial GAN project commit'\")\n",
    "    print(\"   git branch -M main\")\n",
    "    print(\"   git remote add origin https://github.com/USERNAME/REPO_NAME.git\")\n",
    "    print(\"   git push -u origin main\")\n",
    "    \n",
    "    print(\"\\n3Ô∏è‚É£ COLAB USAGE (Every session):\")\n",
    "    print(\"   !git clone https://github.com/USERNAME/REPO_NAME.git\")\n",
    "    print(\"   %cd REPO_NAME\")\n",
    "    print(\"   # Run training...\")\n",
    "    print(\"   !git add results/\")\n",
    "    print(\"   !git commit -m 'Add training results'\")\n",
    "    print(\"   !git push\")\n",
    "    \n",
    "    print(\"\\n4Ô∏è‚É£ AUTHENTICATION:\")\n",
    "    print(\"   Option A: Use Personal Access Token\")\n",
    "    print(\"   Option B: Use GitHub CLI: gh auth login\")\n",
    "    print(\"   Option C: SSH keys (more secure)\")\n",
    "    \n",
    "    print(\"\\nüìù IMPORTANT FILES TO INCLUDE:\")\n",
    "    print(\"   ‚úÖ Source code (src/)\")\n",
    "    print(\"   ‚úÖ Colab notebooks (.ipynb)\")\n",
    "    print(\"   ‚úÖ Requirements (requirements_colab.txt)\")\n",
    "    print(\"   ‚úÖ Documentation (README.md, COLAB_GUIDE.md)\")\n",
    "    print(\"   ‚úÖ Sample dataset (small subset for testing)\")\n",
    "    print(\"   ‚ùå Large datasets (use Git LFS or external storage)\")\n",
    "    print(\"   ‚ùå Trained models (can be optional, use releases)\")\n",
    "\n",
    "# Show the guide\n",
    "show_github_setup_guide()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cafb171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã TEMPLATE COMMANDS FOR YOUR PROJECT\n",
    "# Copy these commands and modify with your GitHub username\n",
    "\n",
    "def generate_setup_commands():\n",
    "    \"\"\"Generate personalized setup commands\"\"\"\n",
    "    \n",
    "    print(\"üìã COPY-PASTE COMMANDS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    github_username = input(\"üë§ Enter your GitHub username: \") or \"YOUR_USERNAME\"\n",
    "    repo_name = input(\"üìÅ Enter repository name: \") or \"GAN_Chili_Disease\"\n",
    "    \n",
    "    print(f\"\\nüîó Your Repository URL: https://github.com/{github_username}/{repo_name}\")\n",
    "    \n",
    "    print(\"\\nüíª LOCAL SETUP (Windows PowerShell):\")\n",
    "    print(f\"\"\"cd \"c:\\Riset Infromatika\\Python V3\\GAN_Project\"\n",
    "git init\n",
    "git add .\n",
    "git commit -m \"Initial GAN project for chili disease classification\"\n",
    "git branch -M main\n",
    "git remote add origin https://github.com/{github_username}/{repo_name}.git\n",
    "git push -u origin main\"\"\")\n",
    "    \n",
    "    print(\"\\nüöÄ COLAB CLONE COMMAND:\")\n",
    "    print(f\"!git clone https://github.com/{github_username}/{repo_name}.git\")\n",
    "    print(f\"%cd {repo_name}\")\n",
    "    \n",
    "    print(\"\\nüíæ PUSH RESULTS COMMAND:\")\n",
    "    print(\"!git add colab_models/ colab_samples/ colab_augmented/\")\n",
    "    print('!git commit -m \"Add Colab training results - $(date)\"')\n",
    "    print(\"!git push origin main\")\n",
    "    \n",
    "    print(\"\\nüìÑ CREATE .gitignore FILE:\")\n",
    "    gitignore_content = \"\"\"# Large files\n",
    "*.pth\n",
    "*.h5\n",
    "*.pkl\n",
    "__pycache__/\n",
    "*.pyc\n",
    "\n",
    "# Large datasets (optional)\n",
    "# Dataset Original/\n",
    "# Dataset Augmented/\n",
    "\n",
    "# Jupyter checkpoints\n",
    ".ipynb_checkpoints/\n",
    "\n",
    "# OS files\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\n",
    "# Results (optional - you may want to track these)\n",
    "# colab_models/\n",
    "# results/\n",
    "# outputs/\"\"\"\n",
    "    \n",
    "    with open('.gitignore', 'w') as f:\n",
    "        f.write(gitignore_content)\n",
    "    \n",
    "    print(\"‚úÖ .gitignore file created!\")\n",
    "    print(\"\\nüìù Note: Adjust .gitignore based on what you want to track\")\n",
    "\n",
    "# Generate commands\n",
    "# generate_setup_commands()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d4559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîê GITHUB AUTHENTICATION IN COLAB\n",
    "# Choose one method for pushing to GitHub\n",
    "\n",
    "def setup_github_auth():\n",
    "    \"\"\"Setup GitHub authentication in Colab\"\"\"\n",
    "    \n",
    "    print(\"üîê GITHUB AUTHENTICATION OPTIONS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"\\nüÖ∞Ô∏è OPTION A: Personal Access Token (Recommended)\")\n",
    "    print(\"   1. Go to GitHub Settings > Developer settings > Personal access tokens\")\n",
    "    print(\"   2. Generate new token (classic)\")\n",
    "    print(\"   3. Select scopes: 'repo' and 'workflow'\")\n",
    "    print(\"   4. Copy the token\")\n",
    "    print(\"   5. Use in Colab:\")\n",
    "    print(\"      !git remote set-url origin https://TOKEN@github.com/USERNAME/REPO.git\")\n",
    "    \n",
    "    print(\"\\nüÖ±Ô∏è OPTION B: GitHub CLI\")\n",
    "    print(\"   !curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg\")\n",
    "    print(\"   !apt update && apt install gh\")\n",
    "    print(\"   !gh auth login\")\n",
    "    \n",
    "    print(\"\\nüÑ≠ OPTION C: SSH Keys (Most Secure)\")\n",
    "    print(\"   # Generate SSH key in Colab\")\n",
    "    print(\"   !ssh-keygen -t ed25519 -C 'your.email@example.com'\")\n",
    "    print(\"   !cat ~/.ssh/id_ed25519.pub\")\n",
    "    print(\"   # Copy public key to GitHub Settings > SSH keys\")\n",
    "    print(\"   !git remote set-url origin git@github.com:USERNAME/REPO.git\")\n",
    "    \n",
    "    print(\"\\nüåü QUICK SETUP (Replace with your info):\")\n",
    "    \n",
    "    choice = input(\"Choose authentication method (A/B/C): \").upper()\n",
    "    \n",
    "    if choice == 'A':\n",
    "        token = input(\"üîë Enter your GitHub token: \")\n",
    "        username = input(\"üë§ Enter your GitHub username: \")\n",
    "        repo = input(\"üìÅ Enter repository name: \")\n",
    "        \n",
    "        print(f\"\\nüìù Run this command:\")\n",
    "        print(f\"!git remote set-url origin https://{token}@github.com/{username}/{repo}.git\")\n",
    "        \n",
    "    elif choice == 'B':\n",
    "        print(\"\\nüìù Installing GitHub CLI...\")\n",
    "        !curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg\n",
    "        !echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main\" | sudo tee /etc/apt/sources.list.d/github-cli.list > /dev/null\n",
    "        !apt update && apt install gh -y\n",
    "        print(\"‚úÖ GitHub CLI installed! Run: !gh auth login\")\n",
    "        \n",
    "    elif choice == 'C':\n",
    "        email = input(\"üìß Enter your email: \")\n",
    "        print(f\"\\nüìù Run these commands:\")\n",
    "        print(f\"!ssh-keygen -t ed25519 -C '{email}' -f ~/.ssh/id_ed25519 -N ''\")\n",
    "        print(\"!cat ~/.ssh/id_ed25519.pub\")\n",
    "        print(\"# Copy the output to GitHub Settings > SSH keys\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Invalid choice. Please run again.\")\n",
    "\n",
    "# Setup authentication\n",
    "# setup_github_auth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c5a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è AUTOMATED GITHUB WORKFLOW\n",
    "# Complete workflow for GitHub integration\n",
    "\n",
    "def complete_github_workflow():\n",
    "    \"\"\"Complete automated GitHub workflow\"\"\"\n",
    "    \n",
    "    print(\"‚öôÔ∏è AUTOMATED GITHUB WORKFLOW\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Step 1: Check if git repo exists\n",
    "    if os.path.exists('.git'):\n",
    "        print(\"‚úÖ Git repository detected\")\n",
    "        \n",
    "        # Pull latest changes\n",
    "        print(\"üîÑ Pulling latest changes...\")\n",
    "        try:\n",
    "            !git pull origin main\n",
    "            print(\"‚úÖ Pull successful\")\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è Pull failed - continuing anyway\")\n",
    "    else:\n",
    "        print(\"‚ùå No git repository found\")\n",
    "        print(\"üìù Please clone your repository first:\")\n",
    "        print(\"   !git clone https://github.com/USERNAME/REPO_NAME.git\")\n",
    "        return False\n",
    "    \n",
    "    # Step 2: Check git status\n",
    "    print(\"\\nüîç Checking git status...\")\n",
    "    !git status\n",
    "    \n",
    "    # Step 3: Setup git config if needed\n",
    "    print(\"\\n‚öôÔ∏è Checking git configuration...\")\n",
    "    try:\n",
    "        username = !git config user.name\n",
    "        email = !git config user.email\n",
    "        \n",
    "        if not username or not email:\n",
    "            print(\"‚ö†Ô∏è Git not configured. Setting up...\")\n",
    "            !git config --global user.name \"Colab User\"\n",
    "            !git config --global user.email \"colab@example.com\"\n",
    "            print(\"‚úÖ Git configured\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Git configured as: {username[0]} <{email[0]}>\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Could not check git config\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def save_and_push_results(commit_message=\"Update from Colab training\"):\n",
    "    \"\"\"Save and push training results to GitHub\"\"\"\n",
    "    \n",
    "    print(f\"\\nüíæ SAVING RESULTS: {commit_message}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Add files\n",
    "    print(\"üìù Adding files to git...\")\n",
    "    \n",
    "    # Add specific directories\n",
    "    files_to_add = [\n",
    "        \"colab_models/\",\n",
    "        \"colab_samples/\", \n",
    "        \"colab_augmented/\",\n",
    "        \"*.png\",\n",
    "        \"*.jpg\",\n",
    "        \"*.json\"\n",
    "    ]\n",
    "    \n",
    "    for file_pattern in files_to_add:\n",
    "        if os.path.exists(file_pattern.rstrip('*')) or '*' in file_pattern:\n",
    "            try:\n",
    "                !git add {file_pattern}\n",
    "                print(f\"  ‚úÖ Added {file_pattern}\")\n",
    "            except:\n",
    "                print(f\"  ‚ö†Ô∏è Could not add {file_pattern}\")\n",
    "    \n",
    "    # Commit\n",
    "    print(f\"\\nüìã Committing changes: {commit_message}\")\n",
    "    try:\n",
    "        !git commit -m \"{commit_message}\"\n",
    "        print(\"‚úÖ Commit successful\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Commit failed - maybe no changes to commit\")\n",
    "    \n",
    "    # Push\n",
    "    print(\"\\nüöÄ Pushing to GitHub...\")\n",
    "    try:\n",
    "        !git push origin main\n",
    "        print(\"‚úÖ Push successful!\")\n",
    "        print(\"üéâ Results are now available on GitHub!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Push failed: {e}\")\n",
    "        print(\"üìù You may need to setup authentication first\")\n",
    "        return False\n",
    "\n",
    "# Run the workflow check\n",
    "workflow_ready = complete_github_workflow()\n",
    "\n",
    "if workflow_ready:\n",
    "    print(\"\\nüéÜ GitHub workflow is ready!\")\n",
    "    print(\"üìù After training, use: save_and_push_results('Training completed')\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Please setup GitHub repository first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä MONITORING & AUTOMATED BACKUP\n",
    "# Automatically backup training progress\n",
    "\n",
    "import time\n",
    "import threading\n",
    "\n",
    "def auto_backup_training(interval_minutes=30):\n",
    "    \"\"\"Automatically backup training progress every X minutes\"\"\"\n",
    "    \n",
    "    def backup_worker():\n",
    "        while True:\n",
    "            time.sleep(interval_minutes * 60)  # Convert to seconds\n",
    "            \n",
    "            print(f\"\\nüîÑ AUTO-BACKUP at {time.strftime('%H:%M:%S')}\")\n",
    "            \n",
    "            # Check if there are any new results\n",
    "            result_dirs = ['colab_models', 'colab_samples', 'colab_augmented']\n",
    "            has_new_results = any(os.path.exists(d) and os.listdir(d) for d in result_dirs)\n",
    "            \n",
    "            if has_new_results:\n",
    "                timestamp = time.strftime('%Y-%m-%d_%H-%M')\n",
    "                commit_msg = f\"Auto-backup training progress - {timestamp}\"\n",
    "                \n",
    "                success = save_and_push_results(commit_msg)\n",
    "                if success:\n",
    "                    print(f\"‚úÖ Auto-backup successful at {timestamp}\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Auto-backup failed at {timestamp}\")\n",
    "            else:\n",
    "                print(\"üìã No new results to backup\")\n",
    "    \n",
    "    # Start backup thread\n",
    "    backup_thread = threading.Thread(target=backup_worker, daemon=True)\n",
    "    backup_thread.start()\n",
    "    \n",
    "    print(f\"üîÑ Auto-backup started! Will backup every {interval_minutes} minutes\")\n",
    "    print(\"üìù This will run in background during training\")\n",
    "    \n",
    "    return backup_thread\n",
    "\n",
    "def manual_backup_now():\n",
    "    \"\"\"Manual backup right now\"\"\"\n",
    "    timestamp = time.strftime('%Y-%m-%d_%H-%M')\n",
    "    commit_msg = f\"Manual backup - {timestamp}\"\n",
    "    \n",
    "    print(f\"\\nüíæ MANUAL BACKUP - {timestamp}\")\n",
    "    success = save_and_push_results(commit_msg)\n",
    "    \n",
    "    if success:\n",
    "        print(f\"üéâ Manual backup completed successfully!\")\n",
    "    else:\n",
    "        print(f\"‚ùå Manual backup failed\")\n",
    "    \n",
    "    return success\n",
    "\n",
    "def show_backup_status():\n",
    "    \"\"\"Show current backup status\"\"\"\n",
    "    print(\"\\nüìä BACKUP STATUS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Check git status\n",
    "    if os.path.exists('.git'):\n",
    "        print(\"üîó Git repository: Active\")\n",
    "        \n",
    "        # Show recent commits\n",
    "        print(\"\\nüìÖ Recent commits:\")\n",
    "        !git log --oneline -5\n",
    "        \n",
    "        # Show current changes\n",
    "        print(\"\\nüìù Current changes:\")\n",
    "        !git status --porcelain\n",
    "        \n",
    "        # Show remote URL\n",
    "        print(\"\\nüåê Remote repository:\")\n",
    "        !git remote -v\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No git repository found\")\n",
    "    \n",
    "    # Check result directories\n",
    "    print(\"\\nüìÅ Result directories:\")\n",
    "    result_dirs = {\n",
    "        'colab_models': 'Trained models',\n",
    "        'colab_samples': 'Training samples', \n",
    "        'colab_augmented': 'Generated images'\n",
    "    }\n",
    "    \n",
    "    for dir_name, description in result_dirs.items():\n",
    "        if os.path.exists(dir_name):\n",
    "            file_count = len([f for f in os.listdir(dir_name) if os.path.isfile(os.path.join(dir_name, f))])\n",
    "            print(f\"  ‚úÖ {dir_name:18}: {file_count} files ({description})\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå {dir_name:18}: Not found\")\n",
    "\n",
    "# Show current status\n",
    "show_backup_status()\n",
    "\n",
    "print(\"\\nüöÄ BACKUP COMMANDS:\")\n",
    "print(\"   manual_backup_now()           - Backup right now\")\n",
    "print(\"   auto_backup_training(30)      - Auto-backup every 30 minutes\")\n",
    "print(\"   show_backup_status()          - Check current status\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
